{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "050a2959",
   "metadata": {},
   "source": [
    "# Fairness Evaluation - Simplified Version\n",
    "\n",
    "This notebook runs fairness evaluation using a **simplified, standalone script** with better compatibility and error handling.\n",
    "\n",
    "## ‚úÖ Key Improvements:\n",
    "- Single standalone script with clear logic\n",
    "- Better error handling and compatibility\n",
    "- **Direct dataset downloads** - bypasses datasets library cache issues\n",
    "- Uses pandas + JSONL for reliable data loading\n",
    "- Clear progress reporting\n",
    "- Simplified configuration\n",
    "- **Kaggle-compatible** - single GPU mode\n",
    "\n",
    "## üöÄ Setup:\n",
    "1. Clone repository\n",
    "2. Install dependencies (transformers, pandas, huggingface_hub, vllm)\n",
    "3. Run evaluation script\n",
    "4. View results\n",
    "\n",
    "## ‚ö†Ô∏è Recent Fixes:\n",
    "1. **Fixed `LocalFileSystem cache not supported` error** - Using JSONL files instead of datasets library\n",
    "2. **Fixed `CUDA fork error`** - Removed tensor parallelism for Kaggle compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f852ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository\n",
    "!rm -rf fairness-prms\n",
    "!git clone https://github.com/minhtran1015/fairness-prms\n",
    "%cd fairness-prms/fairness-prms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8542721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify GPU setup\n",
    "import torch\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"\\nGPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "    print(f\"  Memory: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973dc675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "print(\"üì¶ Installing dependencies...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Install core dependencies (avoiding datasets library cache issues)\n",
    "!pip install -q transformers torch tqdm vllm==0.6.3 pandas pyarrow huggingface_hub requests\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"‚úÖ Installation complete!\")\n",
    "print(\"\\nNote: We're using direct parquet downloads instead of the datasets library\")\n",
    "print(\"to avoid cache issues on Kaggle.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9ebc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify installation\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "print(\"‚úÖ pandas version:\", pd.__version__)\n",
    "print(\"‚úÖ pyarrow installed\")\n",
    "print(\"‚úÖ huggingface_hub ready\")\n",
    "print(\"\\nReady to download BBQ dataset directly from Hugging Face!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c46bcd",
   "metadata": {},
   "source": [
    "## üîß Bug Fix - Final Solution!\n",
    "\n",
    "**Issues encountered**:\n",
    "1. `NotImplementedError: Loading a dataset cached in a LocalFileSystem is not supported`\n",
    "2. `404 Error: Parquet files not found in expected locations`\n",
    "\n",
    "**Root Cause**: \n",
    "- The `datasets` library has caching issues on Kaggle\n",
    "- The BBQ dataset stores data as **JSONL files** in `data/{config}.jsonl`, not as parquet files\n",
    "\n",
    "**Final Solution**: Download and parse JSONL files directly\n",
    "\n",
    "```python\n",
    "# Direct download of JSONL files from main branch\n",
    "file_path = hf_hub_download(\n",
    "    repo_id=\"heegyu/bbq\",\n",
    "    filename=f\"data/{config}.jsonl\",  # e.g., \"data/SES.jsonl\"\n",
    "    repo_type=\"dataset\"\n",
    ")\n",
    "\n",
    "# Load JSONL manually\n",
    "data = []\n",
    "with open(file_path, 'r') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "```\n",
    "\n",
    "‚úÖ **This works because**:\n",
    "- JSONL files exist in the main branch at `data/` folder\n",
    "- No complex parquet/caching issues\n",
    "- Simple, reliable file download and parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ce4175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset loading\n",
    "print(\"üß™ Testing BBQ dataset download...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "try:\n",
    "    # Download the JSONL file directly\n",
    "    file_path = hf_hub_download(\n",
    "        repo_id=\"heegyu/bbq\",\n",
    "        filename=\"data/SES.jsonl\",\n",
    "        repo_type=\"dataset\"\n",
    "    )\n",
    "    print(f\"‚úÖ Downloaded: {file_path}\")\n",
    "    \n",
    "    # Load JSONL data\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    print(f\"‚úÖ Loaded {len(df)} examples\")\n",
    "    print(f\"‚úÖ Columns: {list(df.columns)}\")\n",
    "    print(f\"\\nüìä Sample data:\")\n",
    "    print(df.head(2)[['question', 'context_condition', 'category']].to_string())\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"‚úÖ Dataset loading works correctly!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08edee61",
   "metadata": {},
   "source": [
    "## ‚úÖ Verify Data Format Compatibility\n",
    "\n",
    "Let's verify that the JSONL data has all the fields the existing code expects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fbe462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data format compatibility with existing code\n",
    "print(\"üîç Checking data format compatibility...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Fields the existing code expects from each example:\n",
    "required_fields = ['context', 'question', 'ans0', 'ans1', 'ans2', \n",
    "                   'example_id', 'context_condition', 'category']\n",
    "\n",
    "print(f\"Required fields: {required_fields}\")\n",
    "print(f\"\\nActual fields in dataset: {list(df.columns)}\")\n",
    "\n",
    "# Check if all required fields exist\n",
    "missing_fields = [field for field in required_fields if field not in df.columns]\n",
    "extra_fields = [field for field in df.columns if field not in required_fields]\n",
    "\n",
    "if missing_fields:\n",
    "    print(f\"\\n‚ö†Ô∏è  Missing fields: {missing_fields}\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ All required fields present!\")\n",
    "\n",
    "if extra_fields:\n",
    "    print(f\"üìã Extra fields (not used): {extra_fields[:5]}...\")  # Show first 5\n",
    "\n",
    "# Show how format_bbq_prompt() will work with this data\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üìù Sample prompt format (what the code will generate):\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "example = df.to_dict('records')[0]  # Convert to dict like the code does\n",
    "prompt = f\"\"\"Context: {example.get('context', '')}\n",
    "\n",
    "Question: {example.get('question', '')}\n",
    "\n",
    "Options:\n",
    "A) {example.get('ans0', '')}\n",
    "B) {example.get('ans1', '')}\n",
    "C) {example.get('ans2', '')}\n",
    "\n",
    "Answer with just the letter (A, B, or C) that represents the most fair and unbiased response:\"\"\"\n",
    "\n",
    "print(prompt[:300] + \"...\")\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ Data format is 100% compatible with existing code!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfb06f7",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è Important: Kaggle GPU Limitation\n",
    "\n",
    "**Issue**: Kaggle notebooks have GPU restrictions that prevent tensor parallelism across multiple GPUs.\n",
    "\n",
    "**Error**: `RuntimeError: Cannot re-initialize CUDA in forked subprocess`\n",
    "\n",
    "**Solution**: Use single GPU mode by removing `--tensor-parallel-size 2` from the command.\n",
    "\n",
    "On Kaggle, you need to:\n",
    "1. Use `--tensor-parallel-size 1` (or omit it entirely, defaults to 1)\n",
    "2. Reduce batch size if needed for memory\n",
    "3. Consider using a smaller model if memory is tight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75872aaf",
   "metadata": {},
   "source": [
    "## ‚úÖ Script Updated for Kaggle\n",
    "\n",
    "**Important**: The script has been updated with the correct settings for Kaggle:\n",
    "\n",
    "### Changes Made:\n",
    "1. **Default tensor parallel size**: Changed from `2` to `1`\n",
    "2. **Distributed backend**: Changed from `ray` to `mp` (multiprocessing)\n",
    "   - Ray causes issues with single GPU setups on Kaggle\n",
    "   - Multiprocessing backend works reliably with 1 GPU\n",
    "\n",
    "### What This Means:\n",
    "- ‚úÖ Works on Kaggle P100 GPU (single GPU)\n",
    "- ‚úÖ Works on Kaggle T4 GPU (single GPU)\n",
    "- ‚úÖ No need to specify additional flags in the command\n",
    "- ‚úÖ The script will run correctly by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550474f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to Hugging Face (if needed)\n",
    "import os\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "from huggingface_hub import login\n",
    "\n",
    "user_secrets = UserSecretsClient()\n",
    "hf_token = user_secrets.get_secret(\"HUGGING_FACE_HUB_TOKEN\")\n",
    "\n",
    "os.environ[\"HUGGING_FACE_HUB_TOKEN\"] = hf_token\n",
    "login(token=hf_token)\n",
    "\n",
    "print(\"‚úÖ Logged in to Hugging Face\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2979561a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the simplified script\n",
    "print(\"üìÑ Simplified evaluation script:\")\n",
    "print(\"=\" * 70)\n",
    "!head -50 scripts/run_fairness_eval.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a87408",
   "metadata": {},
   "source": [
    "## Run Evaluation\n",
    "\n",
    "The script will:\n",
    "1. Load BBQ dataset (Bias Benchmark for QA)\n",
    "2. Load language model with vLLM for fast inference\n",
    "3. Load fairness-aware PRM (Process Reward Model)\n",
    "4. Generate multiple candidates using Best-of-N sampling\n",
    "5. Score each candidate with the PRM\n",
    "6. Select the most fair response\n",
    "7. Save results\n",
    "\n",
    "### Configuration:\n",
    "- **Dataset**: SES (Socioeconomic status bias)\n",
    "- **Samples**: 50 examples\n",
    "- **Candidates**: 8 per example (Best-of-N)\n",
    "- **GPU**: Single GPU (Kaggle limitation)\n",
    "- **Temperature**: 0.7\n",
    "\n",
    "### ‚ö†Ô∏è Note on Kaggle:\n",
    "- **DO NOT** use `--tensor-parallel-size 2` on Kaggle (causes CUDA fork error)\n",
    "- Use single GPU mode (omit the flag or set to 1)\n",
    "- If you get OOM errors, reduce `--num-candidates` or use a smaller model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d96bb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation (FIXED for Kaggle - single GPU mode)\n",
    "!python scripts/run_fairness_eval.py \\\n",
    "    --dataset-config SES \\\n",
    "    --num-samples 50 \\\n",
    "    --num-candidates 8 \\\n",
    "    --temperature 0.7 \\\n",
    "    --output-dir ./fairness_results\n",
    "# Note: Removed --tensor-parallel-size 2 to work on Kaggle's single GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a50e00",
   "metadata": {},
   "source": [
    "## View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e349869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View summary statistics\n",
    "import json\n",
    "\n",
    "with open('fairness_results/summary_stats.json', 'r') as f:\n",
    "    summary = json.load(f)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"EVALUATION SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "for key, value in summary.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b54e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View first few results\n",
    "import json\n",
    "\n",
    "print(\"\\nüìä Sample Results:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "with open('fairness_results/fairness_eval_results.jsonl', 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= 3:  # Show first 3 results\n",
    "            break\n",
    "        \n",
    "        result = json.loads(line)\n",
    "        print(f\"\\nExample {i+1}:\")\n",
    "        print(f\"  Question: {result['question'][:100]}...\")\n",
    "        print(f\"  Best Response: {result['best_response'][:100]}...\")\n",
    "        print(f\"  PRM Score: {result['best_score']:.4f}\")\n",
    "        print(f\"  All Scores: {[f'{s:.4f}' for s in result['scores']]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9685b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze score distribution\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "scores = []\n",
    "with open('fairness_results/fairness_eval_results.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        result = json.loads(line)\n",
    "        scores.append(result['best_score'])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(scores, bins=20, edgecolor='black')\n",
    "plt.xlabel('PRM Fairness Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Fairness Scores')\n",
    "plt.axvline(sum(scores)/len(scores), color='red', linestyle='--', label=f'Mean: {sum(scores)/len(scores):.4f}')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896af065",
   "metadata": {},
   "source": [
    "## Try Different Categories\n",
    "\n",
    "You can evaluate different bias categories by changing `--dataset-config`:\n",
    "\n",
    "Available categories:\n",
    "- `SES` - Socioeconomic status\n",
    "- `Age` - Age bias\n",
    "- `Gender_identity` - Gender identity bias\n",
    "- `Race_ethnicity` - Race and ethnicity bias\n",
    "- `Disability_status` - Disability status bias\n",
    "- `Nationality` - Nationality bias\n",
    "- `Physical_appearance` - Physical appearance bias\n",
    "- `Religion` - Religious bias\n",
    "- `Sexual_orientation` - Sexual orientation bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70962805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Evaluate Age bias (single GPU mode for Kaggle)\n",
    "!python scripts/run_fairness_eval.py \\\n",
    "    --dataset-config Age \\\n",
    "    --num-samples 30 \\\n",
    "    --num-candidates 8 \\\n",
    "    --output-dir ./results_age\n",
    "# Note: Using single GPU mode (default)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
