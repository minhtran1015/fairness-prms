{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "050a2959",
   "metadata": {},
   "source": [
    "# Fairness Evaluation - Simplified Version\n",
    "\n",
    "This notebook runs fairness evaluation using a **simplified, standalone script** with better compatibility and error handling.\n",
    "\n",
    "## âœ… Key Improvements:\n",
    "- Single standalone script with clear logic\n",
    "- Better error handling and compatibility\n",
    "- Works with multiple datasets library versions\n",
    "- Clear progress reporting\n",
    "- Simplified configuration\n",
    "\n",
    "## ðŸš€ Setup:\n",
    "1. Clone repository\n",
    "2. Install dependencies \n",
    "3. Run evaluation script\n",
    "4. View results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f852ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository\n",
    "!rm -rf fairness-prms\n",
    "!git clone https://github.com/minhtran1015/fairness-prms\n",
    "%cd fairness-prms/fairness-prms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8542721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify GPU setup\n",
    "import torch\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"\\nGPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "    print(f\"  Memory: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973dc675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "print(\"ðŸ“¦ Installing dependencies...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Install datasets 2.14.0 for best compatibility\n",
    "!pip uninstall -y datasets 2>/dev/null || true\n",
    "!pip install -q datasets==2.14.0\n",
    "\n",
    "# Install other dependencies\n",
    "!pip install -q transformers torch tqdm vllm==0.6.3\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"âœ… Installation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9ebc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify datasets version\n",
    "import datasets\n",
    "print(f\"datasets version: {datasets.__version__}\")\n",
    "\n",
    "if datasets.__version__.startswith('2.'):\n",
    "    print(\"âœ… Compatible datasets version\")\n",
    "else:\n",
    "    print(\"âš ï¸  Warning: Recommended version is 2.14.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550474f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to Hugging Face (if needed)\n",
    "import os\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "from huggingface_hub import login\n",
    "\n",
    "user_secrets = UserSecretsClient()\n",
    "hf_token = user_secrets.get_secret(\"HUGGING_FACE_HUB_TOKEN\")\n",
    "\n",
    "os.environ[\"HUGGING_FACE_HUB_TOKEN\"] = hf_token\n",
    "login(token=hf_token)\n",
    "\n",
    "print(\"âœ… Logged in to Hugging Face\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2979561a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the simplified script\n",
    "print(\"ðŸ“„ Simplified evaluation script:\")\n",
    "print(\"=\" * 70)\n",
    "!head -50 scripts/run_fairness_eval.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a87408",
   "metadata": {},
   "source": [
    "## Run Evaluation\n",
    "\n",
    "The script will:\n",
    "1. Load BBQ dataset (Bias Benchmark for QA)\n",
    "2. Load language model with vLLM for fast inference\n",
    "3. Load fairness-aware PRM (Process Reward Model)\n",
    "4. Generate multiple candidates using Best-of-N sampling\n",
    "5. Score each candidate with the PRM\n",
    "6. Select the most fair response\n",
    "7. Save results\n",
    "\n",
    "### Configuration:\n",
    "- **Dataset**: SES (Socioeconomic status bias)\n",
    "- **Samples**: 50 examples\n",
    "- **Candidates**: 8 per example (Best-of-N)\n",
    "- **GPUs**: 2 T4 GPUs with tensor parallelism\n",
    "- **Temperature**: 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d96bb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation\n",
    "!python scripts/run_fairness_eval.py \\\n",
    "    --dataset-config SES \\\n",
    "    --num-samples 50 \\\n",
    "    --num-candidates 8 \\\n",
    "    --temperature 0.7 \\\n",
    "    --tensor-parallel-size 2 \\\n",
    "    --output-dir ./fairness_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a50e00",
   "metadata": {},
   "source": [
    "## View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e349869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View summary statistics\n",
    "import json\n",
    "\n",
    "with open('fairness_results/summary_stats.json', 'r') as f:\n",
    "    summary = json.load(f)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"EVALUATION SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "for key, value in summary.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b54e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View first few results\n",
    "import json\n",
    "\n",
    "print(\"\\nðŸ“Š Sample Results:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "with open('fairness_results/fairness_eval_results.jsonl', 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= 3:  # Show first 3 results\n",
    "            break\n",
    "        \n",
    "        result = json.loads(line)\n",
    "        print(f\"\\nExample {i+1}:\")\n",
    "        print(f\"  Question: {result['question'][:100]}...\")\n",
    "        print(f\"  Best Response: {result['best_response'][:100]}...\")\n",
    "        print(f\"  PRM Score: {result['best_score']:.4f}\")\n",
    "        print(f\"  All Scores: {[f'{s:.4f}' for s in result['scores']]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9685b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze score distribution\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "scores = []\n",
    "with open('fairness_results/fairness_eval_results.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        result = json.loads(line)\n",
    "        scores.append(result['best_score'])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(scores, bins=20, edgecolor='black')\n",
    "plt.xlabel('PRM Fairness Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Fairness Scores')\n",
    "plt.axvline(sum(scores)/len(scores), color='red', linestyle='--', label=f'Mean: {sum(scores)/len(scores):.4f}')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896af065",
   "metadata": {},
   "source": [
    "## Try Different Categories\n",
    "\n",
    "You can evaluate different bias categories by changing `--dataset-config`:\n",
    "\n",
    "Available categories:\n",
    "- `SES` - Socioeconomic status\n",
    "- `Age` - Age bias\n",
    "- `Gender_identity` - Gender identity bias\n",
    "- `Race_ethnicity` - Race and ethnicity bias\n",
    "- `Disability_status` - Disability status bias\n",
    "- `Nationality` - Nationality bias\n",
    "- `Physical_appearance` - Physical appearance bias\n",
    "- `Religion` - Religious bias\n",
    "- `Sexual_orientation` - Sexual orientation bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70962805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Evaluate Age bias\n",
    "!python scripts/run_fairness_eval.py \\\n",
    "    --dataset-config Age \\\n",
    "    --num-samples 30 \\\n",
    "    --num-candidates 8 \\\n",
    "    --tensor-parallel-size 2 \\\n",
    "    --output-dir ./results_age"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
