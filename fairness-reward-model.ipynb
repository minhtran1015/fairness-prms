{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-07T02:13:51.603952Z",
     "iopub.status.busy": "2025-10-07T02:13:51.603626Z",
     "iopub.status.idle": "2025-10-07T02:13:52.491978Z",
     "shell.execute_reply": "2025-10-07T02:13:52.490601Z",
     "shell.execute_reply.started": "2025-10-07T02:13:51.603917Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Clone the repository (force clean clone if it exists)\n",
    "!rm -rf fairness-prms\n",
    "!git clone https://github.com/minhtran1015/fairness-prms\n",
    "\n",
    "# Verify the fix is present\n",
    "!grep -n \"trust_remote_code\" fairness-prms/fairness-prms/src/sal/utils/data.py || echo \"⚠️ Fix not found!\"\n",
    "\n",
    "# Apply runtime patch to fix Transformers ALL_PARALLEL_STYLES issue\n",
    "print(\"\\n🔧 Applying Transformers patch...\")\n",
    "patch_file = 'fairness-prms/fairness-prms/src/sal/models/prm/transformers_patch.py'\n",
    "patch_code = '''\"\"\"Patch for Transformers ALL_PARALLEL_STYLES issue\"\"\"\n",
    "import transformers.modeling_utils\n",
    "\n",
    "# Fix the NoneType ALL_PARALLEL_STYLES issue\n",
    "# Set to the standard list of supported parallel styles\n",
    "if not hasattr(transformers.modeling_utils, 'ALL_PARALLEL_STYLES') or transformers.modeling_utils.ALL_PARALLEL_STYLES is None:\n",
    "    transformers.modeling_utils.ALL_PARALLEL_STYLES = ['colwise', 'rowwise', 'layerwise']\n",
    "'''\n",
    "\n",
    "with open(patch_file, 'w') as f:\n",
    "    f.write(patch_code)\n",
    "\n",
    "print(\"✅ Transformers patch created\")\n",
    "print(\"   Supported parallel styles: colwise, rowwise, layerwise\")\n",
    "print(\"   This will be imported before model loading\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EARLY CHECK: Verify the code fix is present\n",
    "import sys\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"CHECK 1: Verifying code fix\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "data_file = '/kaggle/working/fairness-prms/fairness-prms/src/sal/utils/data.py'\n",
    "\n",
    "try:\n",
    "    with open(data_file, 'r') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    if 'trust_remote_code=True' in content:\n",
    "        print(\"✅ Code includes trust_remote_code=True\")\n",
    "        print(\"\\n⚠️  Note: You still need to install datasets==2.14.0 (see cell 8)\")\n",
    "    else:\n",
    "        print(\"❌ CRITICAL ERROR: Code does NOT include trust_remote_code=True\")\n",
    "        print(\"   Fix: Update GitHub repo with the fix, then re-run cell 1\")\n",
    "        sys.exit(1)\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(\"❌ ERROR: Could not find data.py file!\")\n",
    "    print(\"   Make sure you ran cell 1 to clone the repository.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK 2: Verify Transformers patch was created\n",
    "import sys\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"CHECK 2: Verifying Transformers patch\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "patch_file = '/kaggle/working/fairness-prms/fairness-prms/src/sal/models/prm/transformers_patch.py'\n",
    "bias_detection_file = '/kaggle/working/fairness-prms/fairness-prms/src/sal/models/prm/bias_detection.py'\n",
    "\n",
    "try:\n",
    "    # Check if patch file exists\n",
    "    with open(patch_file, 'r') as f:\n",
    "        patch_content = f.read()\n",
    "    \n",
    "    if 'ALL_PARALLEL_STYLES' in patch_content:\n",
    "        print(\"✅ Transformers patch file created successfully\")\n",
    "    else:\n",
    "        print(\"❌ CRITICAL ERROR: Patch file exists but doesn't contain fix\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # Check if bias_detection.py imports the patch\n",
    "    with open(bias_detection_file, 'r') as f:\n",
    "        bias_detection_content = f.read()\n",
    "    \n",
    "    if 'from . import transformers_patch' in bias_detection_content:\n",
    "        print(\"✅ bias_detection.py imports the patch\")\n",
    "        print(\"\\n🎉 ALL PATCHES VERIFIED - Safe to continue!\")\n",
    "    else:\n",
    "        print(\"❌ CRITICAL ERROR: bias_detection.py does NOT import patch\")\n",
    "        print(\"   Solution: Update GitHub repo with latest changes\")\n",
    "        sys.exit(1)\n",
    "        \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"❌ ERROR: Could not find required file: {e}\")\n",
    "    print(\"   Make sure you ran cell 1 to clone and patch the repository.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fairness PRMs Kaggle Run\n",
    "\n",
    "This notebook runs a fairness evaluation using Process Reward Models (PRMs) on dual T4 GPUs.\n",
    "\n",
    "## ⚠️ Critical Requirements:\n",
    "\n",
    "The evaluation requires THREE critical fixes:\n",
    "1. **Code fix**: `trust_remote_code=True` in data.py (in GitHub repo)\n",
    "2. **Library fix**: `datasets==2.14.0` (newer versions removed this feature)\n",
    "3. **Transformers patch**: Fix for `ALL_PARALLEL_STYLES` NoneType error (created in cell 1)\n",
    "\n",
    "## 📋 Correct Execution Order:\n",
    "\n",
    "1. **Cell 1**: Clone repository + create Transformers patch\n",
    "2. **Cell 2**: ✅ CHECK 1 - Verify trust_remote_code fix\n",
    "3. **Cell 3**: ✅ CHECK 2 - Verify Transformers patch\n",
    "4. **Cell 5-8**: Setup (list files, change dir, install tree)\n",
    "5. **Cell 9**: 🔧 Install dependencies including `datasets==2.14.0`\n",
    "6. **Cell 10**: ✅ CHECK 3 - Verify datasets version (MUST be 2.14.0)\n",
    "7. **Cell 12-15**: Run evaluation and inspect results\n",
    "\n",
    "⚠️ **Important**: Run cells IN ORDER! All checks must pass before proceeding.\n",
    "\n",
    "## 🚀 Key Features:\n",
    "- Dual T4 GPU support with tensor parallelism\n",
    "- Fairness-aware Best-of-N sampling\n",
    "- Process Reward Model (PRM) for bias detection\n",
    "- Optimized for Kaggle environment\n",
    "- Comprehensive error checking and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T02:13:52.493410Z",
     "iopub.status.busy": "2025-10-07T02:13:52.493132Z",
     "iopub.status.idle": "2025-10-07T02:13:52.629370Z",
     "shell.execute_reply": "2025-10-07T02:13:52.628101Z",
     "shell.execute_reply.started": "2025-10-07T02:13:52.493381Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!ls /kaggle/working/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T02:13:52.632279Z",
     "iopub.status.busy": "2025-10-07T02:13:52.631924Z",
     "iopub.status.idle": "2025-10-07T02:13:52.640266Z",
     "shell.execute_reply": "2025-10-07T02:13:52.639195Z",
     "shell.execute_reply.started": "2025-10-07T02:13:52.632235Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%cd /kaggle/working/fairness-prms/fairness-prms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-10-07T02:13:52.645044Z",
     "iopub.status.busy": "2025-10-07T02:13:52.644698Z",
     "iopub.status.idle": "2025-10-07T02:14:07.231128Z",
     "shell.execute_reply": "2025-10-07T02:14:07.229830Z",
     "shell.execute_reply.started": "2025-10-07T02:13:52.644994Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!apt-get -qq update && apt-get -qq install -y tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T02:14:07.232939Z",
     "iopub.status.busy": "2025-10-07T02:14:07.232559Z",
     "iopub.status.idle": "2025-10-07T02:14:07.357559Z",
     "shell.execute_reply": "2025-10-07T02:14:07.356232Z",
     "shell.execute_reply.started": "2025-10-07T02:14:07.232894Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-10-07T02:14:07.359150Z",
     "iopub.status.busy": "2025-10-07T02:14:07.358784Z",
     "iopub.status.idle": "2025-10-07T02:21:07.528127Z",
     "shell.execute_reply": "2025-10-07T02:21:07.525818Z",
     "shell.execute_reply.started": "2025-10-07T02:14:07.359117Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Install dependencies with specific versions\n",
    "# CRITICAL: Use datasets 2.14.0 which still supports trust_remote_code\n",
    "\n",
    "print(\"🔧 Installing dependencies...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# First, uninstall the current datasets to force downgrade\n",
    "!pip uninstall -y datasets 2>/dev/null || true\n",
    "\n",
    "# Install specific versions\n",
    "!pip install --quiet 'protobuf<4'\n",
    "!pip install --upgrade pip --quiet\n",
    "\n",
    "# Install datasets 2.14.0 FIRST before other packages\n",
    "!pip install --quiet 'datasets==2.14.0'\n",
    "\n",
    "# Then install other dependencies (including latex2sympy2 and word2number for math utils)\n",
    "!pip install --quiet 'protobuf<4' huggingface_hub regex sympy peft numpy latex2sympy2 word2number\n",
    "\n",
    "# Install vLLM 0.6.3 (works well with T4 GPUs - compute capability 7.5)\n",
    "!pip install --quiet 'vllm==0.6.3'\n",
    "\n",
    "# Install project in editable mode (this might try to upgrade datasets, so we pin it)\n",
    "!pip install -e '.[dev]' --quiet --no-deps\n",
    "!pip install --quiet 'datasets==2.14.0'  # Ensure it stays at 2.14.0\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"✅ Installation complete!\")\n",
    "print(\"   Next: Run cell 10 to verify datasets version (CHECK 3)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRITICAL CHECK: Verify datasets version after installation\n",
    "import sys\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"CHECK 3: Verifying datasets library version\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "try:\n",
    "    import datasets\n",
    "    version = datasets.__version__\n",
    "    major, minor = map(int, version.split('.')[:2])\n",
    "    \n",
    "    print(f\"Current datasets version: {version}\")\n",
    "    \n",
    "    if major < 3:  # versions 2.x support trust_remote_code\n",
    "        print(f\"✅ datasets version {version} supports trust_remote_code\")\n",
    "        print(\"\\n🎉 ALL CHECKS PASSED - Safe to continue!\")\n",
    "    else:\n",
    "        print(f\"❌ datasets version {version} does NOT support trust_remote_code\")\n",
    "        print(f\"\\n   Current version: {version}\")\n",
    "        print(f\"   Required: <3.0.0 (recommended: 2.14.0)\")\n",
    "        print(\"\\n   SOLUTION: The installation may have failed to downgrade.\")\n",
    "        print(\"   Try running cell 8 again, or restart the kernel and run from cell 1.\")\n",
    "        sys.exit(1)\n",
    "except Exception as e:\n",
    "    print(f\"❌ ERROR checking datasets version: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loading Fix\n",
    "\n",
    "⚠️ **Root Cause**: The BBQ dataset uses a custom loading script (`bbq.py`), which requires `trust_remote_code=True`. However, newer versions of the `datasets` library (3.0+) have **removed** support for `trust_remote_code` entirely.\n",
    "\n",
    "✅ **Solution**: Install `datasets==2.14.0` which still supports `trust_remote_code=True`. This is handled in the installation cell above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dual T4 GPU Setup\n",
    "\n",
    "**Configuration:** Using 2x T4 GPUs (compute capability 7.5)\n",
    "T4 GPUs are well-supported by vLLM and perfect for this task!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify T4 GPU setup\n",
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"\\nGPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "    print(f\"  Compute Capability: {torch.cuda.get_device_capability(i)}\")\n",
    "    print(f\"  Memory: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration for Dual T4 GPUs\n",
    "\n",
    "With 2x T4 GPUs, we can:\n",
    "- Use `tensor_parallel_size=2` to split the model across both GPUs\n",
    "- Handle more samples (50 instead of 20-40)\n",
    "- Use higher `gpu_memory_utilization=0.85` \n",
    "- Run with full `n=8` for Best-of-N sampling\n",
    "- Use `max_tokens=1024` for complete reasoning\n",
    "\n",
    "Each T4 has ~15GB VRAM, giving us ~30GB total for the model and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "from huggingface_hub import login\n",
    "\n",
    "# This is the code Kaggle gave you to get your secret\n",
    "user_secrets = UserSecretsClient()\n",
    "secret_value_0 = user_secrets.get_secret(\"HUGGING_FACE_HUB_TOKEN\")\n",
    "\n",
    "# --- Use the secret by doing the following ---\n",
    "\n",
    "# 1. Set the environment variable that the Hugging Face library looks for\n",
    "os.environ[\"HUGGING_FACE_HUB_TOKEN\"] = secret_value_0\n",
    "\n",
    "# 2. Now, call the login function. It will find the variable and log you in.\n",
    "login()\n",
    "\n",
    "print(\"Successfully logged in to Hugging Face! ✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variables for Transformers to avoid parallel processing issues\n",
    "import os\n",
    "os.environ['TRANSFORMERS_VERBOSITY'] = 'error'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "# Patch Transformers to fix the NoneType ALL_PARALLEL_STYLES issue\n",
    "import transformers.modeling_utils\n",
    "if not hasattr(transformers.modeling_utils, 'ALL_PARALLEL_STYLES') or transformers.modeling_utils.ALL_PARALLEL_STYLES is None:\n",
    "    # Set to an empty list to prevent the TypeError\n",
    "    transformers.modeling_utils.ALL_PARALLEL_STYLES = []\n",
    "    print(\"✅ Patched Transformers ALL_PARALLEL_STYLES\")\n",
    "\n",
    "print(\"✅ Environment variables configured for Transformers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Run with Dual T4 GPU optimized settings:\n",
    "# - tensor_parallel_size=2 to use both T4 GPUs\n",
    "# - Adjusted batch size and parameters for dual GPU setup\n",
    "# - Each T4 has ~15GB memory, so we can handle more samples\n",
    "!python scripts/test_time_compute.py recipes/fairness_example.yaml \\\n",
    "  --output.push_to_hub=false \\\n",
    "  --dataset.num_samples=50 \\\n",
    "  --search.n=8 \\\n",
    "  --search.max_tokens=1024 \\\n",
    "  --search.temperature=0.7 \\\n",
    "  --model.tensor_parallel_size=2 \\\n",
    "  --model.gpu_memory_utilization=0.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect output JSONL (first 3 lines)\n",
    "import glob, json, os\n",
    "paths = glob.glob('data/**/bon_completions.jsonl', recursive=True)\n",
    "print('Found output files:', paths)\n",
    "if paths:\n",
    "    with open(paths[0]) as f:\n",
    "        for i, line in zip(range(3), f):\n",
    "            row = json.loads(line)\n",
    "            keep_keys = [k for k in row.keys() if k in ['problem','pred','scores','completions']]\n",
    "            preview = {k: row[k] for k in keep_keys}\n",
    "            print(json.dumps(preview, indent=2)[:800])"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
